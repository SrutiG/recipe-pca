{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipes PCA\n",
    "### By Brian Kitano\n",
    "\n",
    "Okay, I'm going to use the Epicurious dataset to identify palates common across their recipes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction / Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "1. Download the JSON data\n",
    "2. Parse the JSON to extract recipe names and ingredients with their quantities.\n",
    "3. Create the data matrix M, where each column is a recipe and each row is an ingredient; the entry is the quantity in a normalized and standardized quantity (grams?)\n",
    "4. PCA\n",
    "\n",
    "Bonus: construct the bipartite graph of ingredients to recipes, and then project it down onto a unipartite graph of ingredients where the weight of each edge is the frequency of connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20130\n",
      "20111\n",
      "20111\n"
     ]
    }
   ],
   "source": [
    "# 1. Parse the ingredients to extract recipe names and ingredients with their quantities\n",
    "import json\n",
    "\n",
    "# load in the epicurious set\n",
    "with open('full_format_recipes.json') as f:\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    \n",
    "# print data[0]['ingredients']\n",
    "print len(data)\n",
    "\n",
    "# need to filter out the recipes without a title\n",
    "data = list(filter(lambda recipe: 'title' in recipe.keys(), data)) \n",
    "print len(data)\n",
    "\n",
    "# need to filter out the recipes that don't have ingredients listed\n",
    "data = list(filter(lambda recipe: 'ingredients' in recipe.keys(), data))\n",
    "print len(data)\n",
    "\n",
    "# as a simple means of cleaning everything, let's strip whitespace from all the listings\n",
    "for recipe in data:\n",
    "    ingredients = recipe['ingredients']\n",
    "    for ingredient in ingredients:\n",
    "        ingredient = ingredient.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17775\n"
     ]
    }
   ],
   "source": [
    "# there are duplicate recipes lol\n",
    "# start by making the hash map of title to recipe\n",
    "titleToRecipe = dict()\n",
    "\n",
    "# for a recipe in the dataset\n",
    "for recipe in data:\n",
    "    title = recipe['title']\n",
    "    # if we haven't seen that recipe before, add it to the dictionary\n",
    "    if title not in titleToRecipe.keys():\n",
    "        titleToRecipe[title] = recipe\n",
    "        # otherwise, doesn't matter, ignore it\n",
    "\n",
    "# from then on out, we can only work with the dictionary\n",
    "print len(titleToRecipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180467\n"
     ]
    }
   ],
   "source": [
    "# get all of the ingredient lists as a list of lists\n",
    "cleanedIngredientsLists = [ titleToRecipe[titleToRecipe.keys()[i]]['ingredients'] for i in range(len(titleToRecipe)) ]\n",
    "\n",
    "# flatten this list, which might contain duplicates\n",
    "cleanedIngredients = [ ingredient for ingredientList in cleanedIngredientsLists for ingredient in ingredientList]\n",
    "print len(cleanedIngredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to deal with the redundancy neatly. What we can do is map each original listing to a number, in another dictionary map that number to a processed listing. Then we only have to work with the processed listings and not fuck with the original mapping. We'll need to make a temporary reverse mapping of the set list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82097\n",
      "82097\n"
     ]
    }
   ],
   "source": [
    "# a deduplicated list of ingredients\n",
    "uniqueCleanedIngredients = list(set(cleanedIngredients))\n",
    "print len(uniqueCleanedIngredients)\n",
    "\n",
    "# a temporary map from cleaned ingredient to index\n",
    "uniqueIngredientToIndex = dict(zip(uniqueCleanedIngredients, range(len(uniqueCleanedIngredients))))\n",
    "\n",
    "# now create a map from the original ingredients to these indices\n",
    "originalIngredientToIndex = dict()\n",
    "for ingredient in cleanedIngredients:\n",
    "    originalIngredientToIndex[ingredient] = uniqueIngredientToIndex[ingredient]\n",
    "    \n",
    "print len(originalIngredientToIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there's a mapping of the original listing to the unique listing, so we can safely process the unique list without losing track of where the original ones came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Before we write all of the ingredients to a file, we should do some NLP cleaning. In looking at the results of the first model run, it seems like to be conservative we should remove all the text that occurs in parentheses, as this seems to really mess up the CRF's ability to identify units. One unfortunate consequence is that we'll no longer be able to filter lists using lambdas, but instead replace them with null strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove things in parentheses (use regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82097\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# remove all the text that is inside a parenthesis\n",
    "noParenthesisIngredients = [re.sub('\\s*\\([^)]*\\)', '', ingredient) for ingredient in uniqueCleanedIngredients]\n",
    "\n",
    "print len(noParenthesisIngredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dealing with the word \"plus\"\n",
    "\n",
    "More complex problem. There are lots of ways that \"plus\" is used. Some examples:\n",
    "\n",
    "##### when quantities don't add nicely\n",
    "- \"1/2 cup plus 1 1/2 tablespoons red wine vinegar\"\n",
    "- \"1/2 cup plus 2 tablespoons granola\"\n",
    "- \"1/4 cup plus 1 tablespoon warm water\"\n",
    "- \"1/4 teaspoon plus 1/3 cup sugar\"\n",
    "- \"2 tablespoons plus 1/2 cup chopped fresh dill\"\n",
    "- \"1 tablespoon plus 1/2 teaspoon Dijon mustard\"\n",
    "- \"2/3 cup plus 6 tablespoons coarsely chopped pecans\"\n",
    "- \"1 cup plus 2 tablespoons whole milk\"\n",
    "- \"1 1/2 cups plus 2 tablespoons sugar\"\n",
    "- \"1 1/2 cups plus 2 tablespoons water\"\n",
    "- \"1 tablespoon plus 3/4 teaspoon ground cinnamon\"\n",
    "- \"1 tablespoon plus one teaspoon fresh lemon juice\"\n",
    "\n",
    "These are in a consistent format of UNIT QUANTITY PLUS UNIT QUANTITY INGREDIENT. If we add PLUS as a label, then over the 3k samples we have we might improve, but we might also tag some things as being PLUS when we don't want them to be.\n",
    "\n",
    "##### when there's a suggestion for more on the side (not a lot of errors there)\n",
    "- \"1/4 cup olive oil, plus more for grilling\"\n",
    "- \"5 teaspoons all-purpose flour plus more for dusting\"\n",
    "- \"2 tablespoons drained capers plus more for serving\"\n",
    "- \"1/2 cup freshly grated Parmesan cheese plus additional for passing\"\n",
    "- \"12 rice-paper rounds, plus more in case some tear\"\n",
    "- \"1 can whole tomatoes, plus juice\"\n",
    "- \"1 tablespoon chile oil containing sesame oil plus some of sediment from jar\"\n",
    "\n",
    "These ones seem like i can just remove all the words after the plus.\n",
    "\n",
    "##### other, stupid ones\n",
    "- \"8 cornichons, finely chopped, plus 2 pickled onions from the jar, minced\"\n",
    "- \"1/2 cup oil-packed sun-dried tomatoes, chopped, plus 2 tablespoons tomato oil\"\n",
    "- \"1 tablespoon fresh rosemary leaves or 1 teaspoon crumbled dried, plus rosemary sprigs for garnish\"\n",
    "- \"Juice of 1/4 lime, plus 1 lime wedge for garnish\"\n",
    "- \"1 1/2 cups sugar, plus 1/4 cup mixed with 1 tablespoon cinnamon, on a plate\"\n",
    "- \"1/2 fennel bulb, finely chopped, plus 1 tablespoon finely chopped fronds\"\n",
    "- \"1/4 cup chopped fresh cilantro plus 32 whole fresh cilantro leaves\"\n",
    "- \"6 large celery stalks, thickly sliced, plus 2 1/2 cups 1/2-inch-thick slices\"\n",
    "- \"6 fresh mint leaves plus 1 mint sprig for garnish\"\n",
    "\n",
    "So also there's like a utility function that might need to be taken into account: we really want our data to fit the format nicely of having a name, a unit, and a quantity. \n",
    "\n",
    "A really, really easy way to deal with all of this is just to get rid of all the \"plus\" ingredient listings, which are only ~3000 out of the 83k samples. It might mess up the data but it's easier. Also none of this is training or testing data, this is like actually \"I need this\" data so it's convenient if I just scrap the shitty stuff. It will also probably have come up in other sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82097"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use a regex to tag an igredient any time \"plus\" appears as a word with or without a comma on its own\n",
    "def removePlus(ingredient):\n",
    "    if (re.search(\"\\s*(plus)\\,*\\s*\", ingredient) == None):\n",
    "        return ingredient\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "noPlusListings = [removePlus(ingredient) for ingredient in noParenthesisIngredients]\n",
    "\n",
    "len(noPlusListings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dashes, commas, and other grammar thingies\n",
    "might be worth removing all of that, but not going to yet. \n",
    "\n",
    "##### Asterisks (*)\n",
    "Asterisks appear in two variants:\n",
    "\"2 1/2 pounds Jerusalem artichokes *\" where the asterisk is at the end, and \"*seedless red grapes\" where it's indicating that this is the start of a comment. We can thus remove anything after an asterisk, since it doesn't matter in either case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82097\n"
     ]
    }
   ],
   "source": [
    "# removing all the text after an asterisk\n",
    "asteriskFreeListings = [ re.sub(\"\\*.*\\n*\",'',ingredient) for ingredient in noPlusListings ]\n",
    "print len(asteriskFreeListings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"a\" and \"an\"\n",
    "This probably maps to the number 1 right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### typos\n",
    "\n",
    "like fam what how is that even ugh how do i check for typos here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"or\"\n",
    "we could remove all the tokens after the word \"or\", since it's optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82097\n"
     ]
    }
   ],
   "source": [
    "# remove all the things after an or\n",
    "noOrListings = [ re.sub(\"[^A-z]\\.*\\,*\\s*(or|OR|Or)+.*\",'', ingredient) for ingredient in asteriskFreeListings ]\n",
    "\n",
    "print len(noOrListings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that our mapping methods are still valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 oz extra-sharp reduced-fat Cheddar (made from 2% milk), coarsely grated\n",
      "4 oz extra-sharp reduced-fat Cheddar, coarsely grated\n",
      "\n",
      "\n",
      "12 large fennel bulbs, trimmed, halved lengthwise, cored, sliced crosswise\n",
      "12 large fennel bulbs, trimmed, halved lengthwise, cored, sliced crosswise\n",
      "\n",
      "\n",
      "1 pork tenderloin\n",
      "1 pork tenderloin\n",
      "\n",
      "\n",
      "2 lb medium shrimp in shell (31 to 35 per pound), peeled and deveined\n",
      "2 lb medium shrimp in shell, peeled and deveined\n",
      "\n",
      "\n",
      "2 tablespoon white wine vinegar\n",
      "2 tablespoon white wine vinegar\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's make a function to make our lives easier for doing lookup\n",
    "def getCleanedListingFromOriginalListing(ingredient, uniqueList):\n",
    "    index = originalIngredientToIndex[ingredient]\n",
    "    return uniqueList[index]\n",
    "\n",
    "for i in range(20,25):\n",
    "    print uniqueCleanedIngredients[i]\n",
    "    print getCleanedListingFromOriginalListing( uniqueCleanedIngredients[i] , noOrListings)\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "21\n",
      "21\n",
      "61194\n",
      "22\n",
      "23\n",
      "23\n",
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# now we make a hash map from the cleaned inputs to their index\n",
    "cleanedListingToIndex = dict(zip(noOrListings, range(len(noOrListings))))\n",
    "\n",
    "# show that the originalIndex and the cleanedIndex are the same\n",
    "for i in range(20,25):\n",
    "    originalIngredient = uniqueCleanedIngredients[i]\n",
    "    cleanedIngredient = getCleanedListingFromOriginalListing(originalIngredient, noOrListings)\n",
    "    print cleanedListingToIndex[cleanedIngredient]\n",
    "    print originalIngredientToIndex[originalIngredient]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so how will we get from the modeled stuff to the original recipe?\n",
    "\n",
    "1. map json to listing index\n",
    "\n",
    "1a. map model json to input to model aka cleanedListing\n",
    "\n",
    "1b. map cleanedListing to index\n",
    "\n",
    "2. map original listing to listing index (done)\n",
    "3. reverse map listing index to json\n",
    "\n",
    "and then i think we're good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's write this clean stuff to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the ingredients to a file, which we'll then feed to a model\n",
    "with open('ingredientsList.txt', 'a') as the_file:\n",
    "    for ingredient in noOrListings:\n",
    "        if ingredient != \"\":\n",
    "            asciiOnlyIngredient = \"\".join(i for i in ingredient if ord(i)<128)\n",
    "            ingredientString = asciiOnlyIngredient + \"\\n\"\n",
    "            the_file.write(ingredientString)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemon juice\n",
      "cup\n",
      "1 1/4\n",
      "79911\n"
     ]
    }
   ],
   "source": [
    "# okay, the model ran and i've got the sauce\n",
    "\n",
    "# load in the labeled stuff\n",
    "with open('results.json') as g:\n",
    "    labeledIngredients = json.load(g)\n",
    "    g.close()\n",
    "    \n",
    "print labeledIngredients[0]['name']\n",
    "print labeledIngredients[0]['unit']\n",
    "print labeledIngredients[0]['qty']\n",
    "\n",
    "print len(labeledIngredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "# now we need to normalize all of the units and measures. We'll use milliliters for volume and grams for mass.\n",
    "\n",
    "# first we'll get a list of all the units\n",
    "def containsUnit(i):\n",
    "    if 'unit' in labeledIngredients[i].keys():\n",
    "        return i\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# get all of the indices which contain units\n",
    "unitContainingIndices = [containsUnit(i) for i in range(len(labeledIngredients))]\n",
    "unitContainingIndices = list(set(unitContainingIndices))\n",
    "\n",
    "# get all of the units\n",
    "unitList = [labeledIngredients[i]['unit'] for i in list(set(unitContainingIndices))]\n",
    "\n",
    "# remove duplicates\n",
    "uniqueUnitList = list(set(unitList))\n",
    "\n",
    "print len(uniqueUnitList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre and Post Modeling Cleaning\n",
    "What cleaning should be done before we feed the model, and what cleaning should be done after? Also, should we change our factor functions? \n",
    "\n",
    "Well, let's think quantitatively about what cleaning means now. We've identified the units from the model, and they're obviously not perfect. We should look at whether we can just cut the shitty ones out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary mapping unit to ingredients\n",
    "sortedIngredientsByUnit = dict()\n",
    "\n",
    "for ingredient in labeledIngredients:\n",
    "    unit = 'na'\n",
    "    # if there's a unit associated with the ingredient\n",
    "    if 'unit' in ingredient.keys():\n",
    "        unit = ingredient['unit']\n",
    "    \n",
    "    if isinstance(ingredient, dict):\n",
    "        # if that unit is already in the dictionary\n",
    "        if unit in sortedIngredientsByUnit.keys():\n",
    "            sortedIngredientsByUnit[unit].append(ingredient)\n",
    "        else:\n",
    "            # that unit is unseen, so we need to create it\n",
    "            sortedIngredientsByUnit[unit] = [ingredient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('na', 28673), (u'cup', 20885), (u'tablespoon', 8162), (u'pound', 5929), (u'teaspoon', 5429), (u'ounce', 3723), (u'slice', 1037), (u'clove', 817), (u'bunch', 698), (u'head', 596), (u'piece', 493), (u'can', 458), (u'sprig', 449), (u'stick', 322), (u'stalk', 282), (u'package', 275), (u'pint', 237), (u'quart', 168), (u'pinch', 156), (u'fillet', 147), (u'strip', 131), (u'bottle', 112), (u'ear', 105), (u'dash', 77), (u'jar', 53), (u'bag', 52), (u'handful', 49), (u'loaf', 41), (u'gram', 35), (u'dozen', 31), (u'bulb', 25), (u'envelope', 24), (u'sheet', 23), (u'cup sprig', 22), (u'box', 19), (u'cube', 12), (u'gallon', 11), (u'batch', 10), (u'clove teaspoon', 9), (u'square', 8), (u'knob', 8), (u'rack', 7), (u'pound fillet', 7), (u'ounce slice', 7), (u'wedge', 6), (u'ball', 6), (u'cup tablespoon', 5), (u'chunk', 5), (u'12-ounce', 5), (u'splash', 4), (u'liter', 4), (u'drop', 4), (u'twist', 3), (u'tablespoon tablespoon', 3), (u'log', 3), (u'cup slice', 3), (u'can fillet', 3), (u'teaspoon teaspoon', 2), (u'tablespoon sprig', 2), (u'tablespoon cup', 2), (u'packet', 2), (u'ounce fillet', 2), (u'cup cup', 2), (u'bunch sprig', 2), (u'tablespoon teaspoon', 1), (u'tablespoon stalk', 1), (u'stem', 1), (u'segment', 1), (u'pound stalk', 1), (u'pound slice', 1), (u'pound pound', 1), (u'pound ounce', 1), (u'pound cup', 1), (u'pinch sprig', 1), (u'piece fillet', 1), (u'pair', 1), (u'ounce cup', 1), (u'ounce can', 1), (u'ounce bunch', 1), (u'link', 1), (u'head clove', 1), (u'fifth', 1), (u'cup teaspoon', 1), (u'cup tablespoon cup', 1), (u'cup strip', 1), (u'cup stalk', 1), (u'cup piece', 1), (u'clove clove', 1), (u'bulb tablespoon', 1), (u'branch', 1), (u'12-ounce bunch', 1), (u'12-ounce bottle', 1), (u'12-ounce bag', 1)]\n"
     ]
    }
   ],
   "source": [
    "unitByCount = dict()\n",
    "\n",
    "for unit in sortedIngredientsByUnit.keys():\n",
    "    unitByCount[unit] = len(sortedIngredientsByUnit[unit])\n",
    "    \n",
    "unitByCountSorted = (sorted(unitByCount.iteritems(), key=lambda (k,v): (v,k), reverse=True))\n",
    "\n",
    "print unitByCountSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I think since the first 25 units account for ~99% of the ingredients in the set, I'm just gonna drop the remaining ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'pound': 5929, u'dash': 77, u'strip': 131, u'bunch': 698, u'clove': 817, u'slice': 1037, u'cup': 20885, 'na': 28673, u'jar': 53, u'fillet': 147, u'teaspoon': 5429, u'stalk': 282, u'pint': 237, u'head': 596, u'tablespoon': 8162, u'quart': 168, u'stick': 322, u'ear': 105, u'package': 275, u'pinch': 156, u'ounce': 3723, u'sprig': 449, u'can': 458, u'bottle': 112, u'piece': 493}\n",
      "79414\n"
     ]
    }
   ],
   "source": [
    "unitByCountTruncated = dict(unitByCountSorted[:25])\n",
    "print unitByCountTruncated\n",
    "print sum(unitByCountTruncated.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have four problems: I need to create a mapping of recipe title to its data, I need to match ingredients to recipes, I need to match cleaned ingredient listings to their original ingredients, and I need to convert units to standardized things to compare them with. \n",
    "\n",
    "This first two problem I can just use a hash table, where I map original ingredient listings to the recipe they came from.\n",
    "\n",
    "The third problem I can also use a hash table mapping the original listing to the cleaned listing. It will have to be generated and updated throughout the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83465\n"
     ]
    }
   ],
   "source": [
    "# Hash table mapping original ingredient listing to recipe that they came from\n",
    "ingredientToRecipe = dict()\n",
    "\n",
    "for recipe in data:\n",
    "    if 'ingredients' in recipe.keys():\n",
    "        ingredients = recipe['ingredients']\n",
    "        for ingredient in ingredients:\n",
    "            ingredientToRecipe[ingredient] = recipe['title']\n",
    "    \n",
    "print len(ingredientToRecipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17775\n",
      "2336\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
